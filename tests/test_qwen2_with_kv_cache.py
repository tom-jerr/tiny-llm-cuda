import pytest
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from .utils import *
from .tinyllm_base import (
    Qwen2ModelV2,
    TinyKvFullCache,
)

# --- PyTorch 设备设置 ---
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")


@torch.no_grad()  # 禁用梯度计算
def helper_test_task_3(model_name: str, iters: int = 10):

    torch_model = AutoModelForCausalLM.from_pretrained(
        model_name, torch_dtype=torch.float16, device_map=device
    )
    tokenizer = AutoTokenizer.from_pretrained(model_name)

    model = Qwen2ModelV2(torch_model)
    torch.manual_seed(42)
    for _ in range(iters):
        cache = [TinyKvFullCache() for _ in range(model.num_hidden_layers)]

        input_tensor = torch.randint(
            low=0,
            high=tokenizer.vocab_size,
            size=(1, 10),
            dtype=torch.long,
            device=device,
        )

        user_output = model(input_tensor, 0, cache)
        user_output = user_output - torch.logsumexp(user_output, dim=-1, keepdim=True)

        ref_output = torch_model(input_tensor).logits
        ref_output = ref_output - torch.logsumexp(ref_output, dim=-1, keepdim=True)

        # 假设 assert_allclose 现在使用 torch.allclose
        assert_allclose(
            user_output, ref_output, precision=torch.float16, rtol=0.1, atol=0.5
        )


@pytest.mark.skipif(
    not qwen_2_05b_model_exists(), reason="Qwen2-0.5B-Instruct model not found"
)
def test_task_3_qwen_2_05b():
    helper_test_task_3("Qwen/Qwen2-0.5B-Instruct", 5)


@pytest.mark.skipif(
    not qwen_2_15b_model_exists(), reason="Qwen2-1.5B-Instruct model not found"
)
def test_task_3_qwen_2_15b():
    helper_test_task_3("Qwen/Qwen2-1.5B-Instruct", 3)


@torch.no_grad()  # 禁用梯度计算
def helper_test_task_4(model_name: str, seq_len: int, iters: int = 1):

    torch_model = AutoModelForCausalLM.from_pretrained(
        model_name, torch_dtype=torch.float16, device_map=device
    )
    tokenizer = AutoTokenizer.from_pretrained(model_name)

    model = Qwen2ModelV2(torch_model)
    torch.manual_seed(42)
    for _ in range(iters):
        cache = [TinyKvFullCache() for _ in range(model.num_hidden_layers)]

        input_tensor = torch.randint(
            low=0,
            high=tokenizer.vocab_size,
            size=(1, seq_len),
            dtype=torch.long,
            device=device,
        )

        ref_outputs = torch_model(input_tensor)
        for offset in range(seq_len):
            user_output = model(input_tensor[:, offset : offset + 1], offset, cache)
            user_output = user_output - torch.logsumexp(
                user_output, dim=-1, keepdim=True
            )

            ref_output = ref_outputs.logits[:, offset, :]
            ref_output = ref_output - torch.logsumexp(ref_output, dim=-1, keepdim=True)

            # 假设 assert_allclose 现在使用 torch.allclose
            assert_allclose(user_output, ref_output, precision=torch.float16, rtol=0.1)


@pytest.mark.skipif(
    not qwen_2_05b_model_exists(), reason="Qwen2-0.5B-Instruct model not found"
)
def test_task_4_qwen_2_05b():
    helper_test_task_4("Qwen/Qwen2-0.5B-Instruct", seq_len=3)


@pytest.mark.skipif(
    not qwen_2_15b_model_exists(), reason="Qwen2-1.5B-Instruct model not found"
)
def test_task_4_qwen_2_15b():
    helper_test_task_4("Qwen/Qwen2-1.5B-Instruct", seq_len=3)
