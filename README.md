# MiniInfer

<div align="center">

[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.6+-red.svg)](https://pytorch.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

**ä¸€ä¸ªä»é›¶å¼€å§‹æ„å»ºçš„è½»é‡çº§é«˜æ€§èƒ½ LLM æ¨ç†å¼•æ“**

[ç‰¹æ€§](#ç‰¹æ€§) â€¢ [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹) â€¢ [æ¶æ„](#æ¶æ„) â€¢ [ç¤ºä¾‹](#ç¤ºä¾‹) â€¢ [æ–‡æ¡£](#æ–‡æ¡£)

</div>

---

## ğŸ“ é¡¹ç›®ç®€ä»‹

MiniInfer æ˜¯ä¸€ä¸ªæ•™è‚²æ€§è´¨çš„å¤§è¯­è¨€æ¨¡å‹æ¨ç†å¼•æ“ï¼Œä»é›¶å®ç°äº†ç°ä»£ LLM æ¨ç†ç³»ç»Ÿçš„æ ¸å¿ƒç»„ä»¶ã€‚æœ¬é¡¹ç›®æ—¨åœ¨å¸®åŠ©å¼€å‘è€…æ·±å…¥ç†è§£ LLM æ¨ç†çš„åº•å±‚æœºåˆ¶ï¼ŒåŒ…æ‹¬æ³¨æ„åŠ›æœºåˆ¶ã€KV ç¼“å­˜ã€æ‰¹å¤„ç†è°ƒåº¦ç­‰å…³é”®æŠ€æœ¯ã€‚

### æ ¸å¿ƒç›®æ ‡

- ğŸ¯ **æ•™è‚²å¯¼å‘**ï¼šæ¸…æ™°çš„ä»£ç ç»“æ„å’Œè¯¦ç»†çš„æ³¨é‡Š
- âš¡ **æ€§èƒ½ä¼˜åŒ–**ï¼šå®ç°ä¸»æµæ¨ç†ä¼˜åŒ–æŠ€æœ¯
- ğŸ”§ **æ˜“äºæ‰©å±•**ï¼šæ¨¡å—åŒ–è®¾è®¡ï¼Œä¾¿äºæ·»åŠ æ–°åŠŸèƒ½
- ğŸ“š **å®Œæ•´å®ç°**ï¼šä»åŸºç¡€ç®—å­åˆ°å®Œæ•´æ¨ç†æµç¨‹

## âœ¨ ç‰¹æ€§

### å·²å®ç°åŠŸèƒ½

#### ğŸ§  æ ¸å¿ƒç»„ä»¶

- âœ… **æ³¨æ„åŠ›æœºåˆ¶**
  - Multi-Head Attention (MHA)
  - Grouped Query Attention (GQA)
  - æ”¯æŒå› æœæ©ç  (Causal Mask)
  - å¤šç§å®ç°æ–¹å¼å¯åˆ‡æ¢

- âœ… **ä½ç½®ç¼–ç **
  - Rotary Position Embedding (RoPE)
  - æ”¯æŒä¼ ç»Ÿå’Œéä¼ ç»Ÿæ¨¡å¼ï¼ˆQwen2 ä½¿ç”¨ non-tranditionalï¼‰

- âœ… **æ¿€æ´»å‡½æ•°**
  - SiLU / Swish
  - GELU / ReLU / Leaky ReLU
  - Tanh / Sigmoid
  - ç»Ÿä¸€æ¥å£ï¼Œæ˜“äºæ‰©å±•

- âœ… **å½’ä¸€åŒ–å±‚**
  - RMSNorm

#### ğŸš€ æ¨ç†ä¼˜åŒ–

- âœ… **KV Cache**
  - å•è¯·æ±‚ KV Cache
  - æ‰¹å¤„ç† KV Cache (å°¾éƒ¨å¯¹é½)
  - æ”¯æŒåŠ¨æ€è¯·æ±‚ç®¡ç†

- âœ… **æ‰¹å¤„ç†æ¨ç†**
  - Continuous Batching
  - åŠ¨æ€è¯·æ±‚è°ƒåº¦
  - æ”¯æŒå¤šè¯·æ±‚å¹¶å‘

- âœ… **CUDA æ‰©å±•**
  - C++/CUDA è‡ªå®šä¹‰ç®—å­
  - PyTorch C++ æ‰©å±•æ¡†æ¶
  - å‘é‡åŠ æ³•ç¤ºä¾‹ï¼ˆå¯æ‰©å±•æ›´å¤šç®—å­ï¼‰

#### ğŸ¤– æ¨¡å‹æ”¯æŒ

- âœ… **Qwen2** ç³»åˆ—æ¨¡å‹ (0.5B, 1.5B, 7B)
  > :skull: 7B æœªè¿›è¡Œæµ‹è¯•
  - å®Œæ•´çš„æ¨¡å‹å®ç°
  - æƒé‡åŠ è½½ä¸è½¬æ¢
  - é‡åŒ–æ”¯æŒ (FP16)

### ğŸ”® è®¡åˆ’åŠŸèƒ½ (è¯¦è§ [ROADMAP.md](./ROADMAP.md))

- :construction: é¡¹ç›®ä»£ç é‡æ„ä¸­(**é‡æ„ç›®æ ‡ç±»ä¼¼ nano-vllm**)
- ğŸ”„ å¼ é‡å¹¶è¡Œ (Tensor Parallelism)
- ğŸ”„ æµæ°´çº¿å¹¶è¡Œ (Pipeline Parallelism)
- ğŸ”„ Flash Attention é›†æˆ
- ğŸ”„ PagedAttention
- ğŸ”„ æ¨æµ‹è§£ç  (Speculative Decoding)
- ğŸ”„ é‡åŒ–æ”¯æŒ (INT8/INT4)
- ğŸ”„ æ›´å¤šæ¨¡å‹æ¶æ„

## ğŸš€ å¿«é€Ÿå¼€å§‹

> ğŸ’¡ **æ–°æ‰‹æ¨è**: æŸ¥çœ‹è¯¦ç»†çš„ [å¿«é€Ÿå…¥é—¨æŒ‡å—](./docs/QUICKSTART.md) è·å–å®Œæ•´æ•™ç¨‹å’Œå¸¸è§é—®é¢˜è§£ç­”ã€‚

### ç¯å¢ƒè¦æ±‚

- Python 3.10-3.12
- CUDA 11.8+ (ç”¨äº GPU åŠ é€Ÿ)
- 8GB+ GPU æ˜¾å­˜ï¼ˆæ¨èï¼‰

### å®‰è£…

1. **å…‹éš†ä»“åº“**

```bash
git clone https://github.com/tom-jerr/MiniInfer .git
cd MiniInfer
```

2. **å®‰è£…ä¾èµ–**

ä½¿ç”¨ PDM (æ¨è):

```bash
pip install pdm
pdm install
```

æˆ–ä½¿ç”¨ pip:

```bash
pip install torch>=2.6.0 transformers>=4.51.0 flash-attn>=2.8.3
```

3. **æ„å»º CUDA æ‰©å±•** (å¯é€‰)

```bash
pdm run build-ext
pdm run build-ext-test  # æµ‹è¯•æ‰©å±•
```

### åŸºç¡€ä½¿ç”¨

#### 1. å•ä¸ªè¯·æ±‚æ¨ç†

```bash
# ä½¿ç”¨ç®€å•ç”Ÿæˆ (æ—  KV Cache)
python main.py --model Qwen/Qwen2-1.5B --loader v1 --prompt "ä»‹ç»ä¸€ä¸‹å¤§è¯­è¨€æ¨¡å‹"

# ä½¿ç”¨ KV Cache åŠ é€Ÿ
python main.py --model Qwen/Qwen2-1.5B --loader v2 --prompt "ä»‹ç»ä¸€ä¸‹å¤§è¯­è¨€æ¨¡å‹"
```

#### 2. æ‰¹å¤„ç†æ¨ç†

```bash
python batch-main.py \
  --model Qwen/Qwen2-0.5B-Instruct \
  --batch-size 5 \
  --prefill-step 128 \
  --max-seq-len 512
```

#### 3. è¿è¡Œæµ‹è¯•

```bash
# è¿è¡Œæ‰€æœ‰æµ‹è¯•
pdm run test

# è¿è¡Œç‰¹å®šæµ‹è¯•
python tests/test_attention.py
python tests/test_qwen2.py
python tests/test_batching.py
```

### ä½¿ç”¨ç¤ºä¾‹

æŸ¥çœ‹ `examples/` ç›®å½•è·å–æ›´å¤šç¤ºä¾‹ï¼š

```bash
# æ¿€æ´»å‡½æ•°ä½¿ç”¨ç¤ºä¾‹
python examples/activation_usage.py

# æ³¨æ„åŠ›æœºåˆ¶ä½¿ç”¨ç¤ºä¾‹
python examples/attention_usage.py
```

## ğŸ—ï¸ æ¶æ„

### é¡¹ç›®ç»“æ„

```
MiniInfer /
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ layers/              # ç¥ç»ç½‘ç»œå±‚å®ç°
â”‚   â”‚   â”œâ”€â”€ activation.py    # æ¿€æ´»å‡½æ•°
â”‚   â”‚   â”œâ”€â”€ attention.py     # æ³¨æ„åŠ›æœºåˆ¶
â”‚   â”‚   â”œâ”€â”€ embedding.py     # åµŒå…¥å±‚
â”‚   â”‚   â”œâ”€â”€ layernorm.py     # å½’ä¸€åŒ–å±‚
â”‚   â”‚   â”œâ”€â”€ linear.py        # çº¿æ€§å±‚
â”‚   â”‚   â”œâ”€â”€ position_encoding.py  # ä½ç½®ç¼–ç 
â”‚   â”‚   â””â”€â”€ sampler.py       # é‡‡æ ·å™¨
â”‚   â”œâ”€â”€ models/              # æ¨¡å‹å®ç°
â”‚   â”‚   â”œâ”€â”€ qwen2.py         # Qwen2 æ¨¡å‹
â”‚   â”‚   â””â”€â”€ configs/         # æ¨¡å‹é…ç½®
â”‚   â”œâ”€â”€ cache/               # KV Cache å®ç°
â”‚   â”‚   â”œâ”€â”€ kv_cache.py      # KV Cache æ ¸å¿ƒ
â”‚   â”‚   â”œâ”€â”€ generate.py      # ç”Ÿæˆé€»è¾‘
â”‚   â”‚   â””â”€â”€ request.py       # è¯·æ±‚ç®¡ç†ä¸æ‰¹å¤„ç†
â”‚   â”œâ”€â”€ utils/               # å·¥å…·å‡½æ•°
â”‚   â”‚   â”œâ”€â”€ model_utils.py   # æ¨¡å‹å·¥å…·
â”‚   â”‚   â””â”€â”€ quantize.py      # é‡åŒ–å·¥å…·
â”‚   â””â”€â”€ extensions/          # CUDA æ‰©å±•
â”‚       â”œâ”€â”€ bindings.cpp     # Python ç»‘å®š
â”‚       â”œâ”€â”€ setup.py         # ç¼–è¯‘é…ç½®
â”‚       â””â”€â”€ ops/             # CUDA ç®—å­
â”‚           â”œâ”€â”€ vector_add.cu
â”‚           â””â”€â”€ vector_add.h
â”œâ”€â”€ tests/                   # æµ‹è¯•æ–‡ä»¶
â”œâ”€â”€ examples/                # ä½¿ç”¨ç¤ºä¾‹
â”œâ”€â”€ main.py                  # å•è¯·æ±‚æ¨ç†å…¥å£
â”œâ”€â”€ batch-main.py            # æ‰¹å¤„ç†æ¨ç†å…¥å£
â””â”€â”€ pyproject.toml           # é¡¹ç›®é…ç½®
```

### æ ¸å¿ƒè®¾è®¡

#### 1. æ¨¡å—åŒ–æ³¨æ„åŠ›å®ç°

æ”¯æŒå¤šç§æ³¨æ„åŠ›å®ç°ï¼Œé€šè¿‡å­—ç¬¦ä¸²é€‰æ‹©ï¼š

```python
from src.layers.attention import scaled_dot_product_attention

# ä½¿ç”¨ GQA
output = scaled_dot_product_attention(
    query, key, value,
    implementation="gqa",
    mask="causal"
)

# ä½¿ç”¨ç®€å•å®ç°
output = scaled_dot_product_attention(
    query, key, value,
    implementation="simple"
)
```

#### 2. çµæ´»çš„ KV Cache

- **å•è¯·æ±‚ Cache**: `TinyKvFullCache` - ç”¨äºå•ä¸ªè¯·æ±‚çš„ KV ç¼“å­˜
- **æ‰¹å¤„ç† Cache**: `BatchingKvCache` - æ”¯æŒå¤šè¯·æ±‚å¹¶å‘ï¼Œå°¾éƒ¨å¯¹é½è®¾è®¡

```python
# æ‰¹å¤„ç† KV Cache å°¾éƒ¨å¯¹é½ç¤ºä¾‹
# batched_keys[i, :, (S-S_i):S, :] = keys[i, :, :, :]
# ä½¿å¾—ä¸åŒé•¿åº¦åºåˆ—å¯ä»¥å…±äº«ç»Ÿä¸€çš„å› æœé®ç½©
```

#### 3. Continuous Batching

å®ç°äº†ç±»ä¼¼ vLLM çš„è¿ç»­æ‰¹å¤„ç†è°ƒåº¦ï¼š

- Prefill å’Œ Decode é˜¶æ®µåˆ†ç¦»
- åŠ¨æ€è¯·æ±‚æ·»åŠ ä¸ç§»é™¤
- é«˜æ•ˆçš„æ‰¹å¤„ç†è°ƒåº¦
- æ”¯æŒå¯é…ç½®çš„ prefill æ­¥é•¿

## ğŸ“Š ç¤ºä¾‹

### æ³¨æ„åŠ›æœºåˆ¶é€‰æ‹©

```python
from src.layers.attention import get_attention, scaled_dot_product_attention

# æ–¹æ³•1: ç›´æ¥è·å–å®ç°
attn_fn = get_attention("gqa")
output = attn_fn(query, key, value, scale=0.125, mask="causal")

# æ–¹æ³•2: ä½¿ç”¨ç»Ÿä¸€æ¥å£
output = scaled_dot_product_attention(
    query, key, value,
    scale=0.125,
    mask="causal",
    implementation="gqa"
)
```

### æ¿€æ´»å‡½æ•°ä½¿ç”¨

```python
from src.layers.activation import get_activation, apply_activation

# æ–¹æ³•1: è·å–æ¿€æ´»å‡½æ•°
silu = get_activation("silu")
output = silu(x)

# æ–¹æ³•2: ç›´æ¥åº”ç”¨
output = apply_activation(x, activation="silu")

# å¸¦å‚æ•°çš„æ¿€æ´»å‡½æ•°
output = apply_activation(x, activation="leaky_relu", negative_slope=0.1)
```

### æ‰¹å¤„ç†æ¨ç†

```python
from src.cache.request import batch_generate
from src.models.qwen2 import Qwen2ModelV2

# å‡†å¤‡å¤šä¸ª prompts
prompts = [
    "What is the capital of France?",
    "ä»‹ç»ä¸€ä¸‹ä¸Šæµ·",
    "Explain quantum computing",
]

# æ‰¹å¤„ç†ç”Ÿæˆ
results = batch_generate(
    model=tiny_llm_model,
    tokenizer=tokenizer,
    prompts=prompts,
    max_seq_len=512,
    batch_size=5,
    prefill_step=128,
)

# è¾“å‡ºç»“æœ
for prompt_idx, text in results:
    print(f"Prompt {prompt_idx}: {text}")
```

## ğŸ§ª æµ‹è¯•

é¡¹ç›®åŒ…å«å®Œæ•´çš„æµ‹è¯•å¥—ä»¶ï¼š

```bash
# è¿è¡Œæ‰€æœ‰æµ‹è¯•
pdm run test

# è¿è¡Œç‰¹å®šæµ‹è¯•
python tests/test_attention.py      # æ³¨æ„åŠ›æœºåˆ¶æµ‹è¯•
python tests/test_gqa.py            # GQA æµ‹è¯•
python tests/test_mha.py            # MHA æµ‹è¯•
python tests/test_rope.py           # RoPE æµ‹è¯•
python tests/test_qwen2.py          # Qwen2 æ¨¡å‹æµ‹è¯•
python tests/test_batching.py       # æ‰¹å¤„ç†æµ‹è¯•
```

## ğŸ› ï¸ å¼€å‘

### PDM è„šæœ¬

```bash
pdm run main         # è¿è¡Œä¸»ç¨‹åº
pdm run main-v1      # ä½¿ç”¨ v1 loader
pdm run main-v2      # ä½¿ç”¨ v2 loader (KV Cache)
pdm run batch-main   # æ‰¹å¤„ç†æ¨ç†
pdm run test         # è¿è¡Œæµ‹è¯•
pdm run format       # ä»£ç æ ¼å¼åŒ–
pdm run build-ext    # æ„å»º CUDA æ‰©å±•
```

### æ·»åŠ æ–°çš„æ¿€æ´»å‡½æ•°

```python
# åœ¨ src/layers/activation.py ä¸­
def my_activation(x: torch.Tensor) -> torch.Tensor:
    return x * torch.sigmoid(x)

# æ³¨å†Œåˆ°å…¨å±€å­—å…¸
ACTIVATION_IMPLEMENTATIONS["my_act"] = my_activation
```

### æ·»åŠ æ–°çš„æ³¨æ„åŠ›å®ç°

```python
# åœ¨ src/layers/attention.py ä¸­
def my_attention(query, key, value, scale=None, mask=None):
    # è‡ªå®šä¹‰å®ç°
    pass

# æ³¨å†Œ
ATTENTION_IMPLEMENTATIONS["my_attn"] = my_attention
```

## ğŸ“ˆ æ€§èƒ½

### æ‰¹å¤„ç†æ•ˆç‡

ä½¿ç”¨ Continuous Batching å¯ä»¥æ˜¾è‘—æå‡ååé‡ï¼š

- å•è¯·æ±‚æ¨ç†: ~10 tokens/s
- æ‰¹å¤„ç† (batch_size=5): ~40 tokens/s (4x æå‡)
- å†…å­˜åˆ©ç”¨ç‡æå‡: 2-3x

### KV Cache åŠ é€Ÿ

- æ—  KV Cache: ~10 tokens/s
- æœ‰ KV Cache: ~30-50 tokens/s (3-5x æå‡)

_æ³¨: æ€§èƒ½æ•°æ®åŸºäº NVIDIA RTX 3090, Qwen2-1.5B æ¨¡å‹_

## ğŸ—ºï¸ å¼€å‘è·¯çº¿

æŸ¥çœ‹ [ROADMAP.md](./ROADMAP.md) äº†è§£è¯¦ç»†çš„å¼€å‘è®¡åˆ’å’Œè¿›åº¦ã€‚

### è¿‘æœŸç›®æ ‡ (Q1 2026)

- [ ] Flash Attention é›†æˆ
- [ ] PagedAttention å®ç°
- [ ] å¼ é‡å¹¶è¡ŒåŸºç¡€æ”¯æŒ

### ä¸­æœŸç›®æ ‡ (Q2-Q3 2026)

- [ ] å®Œæ•´çš„ Tensor Parallelism
- [ ] Pipeline Parallelism
- [ ] INT8 é‡åŒ–æ”¯æŒ

## ğŸ“š æ–‡æ¡£

- **[å¿«é€Ÿå…¥é—¨æŒ‡å—](./docs/QUICKSTART.md)** - 5 åˆ†é’Ÿä¸Šæ‰‹æ•™ç¨‹ï¼ŒåŒ…å«å¸¸è§é—®é¢˜è§£ç­”
- **[å¼€å‘è·¯çº¿å›¾](./ROADMAP.md)** - è¯¦ç»†çš„åŠŸèƒ½è§„åˆ’å’Œå¼€å‘è¿›åº¦
- **[è´¡çŒ®æŒ‡å—](./CONTRIBUTING.md)** - å¦‚ä½•ä¸ºé¡¹ç›®åšè´¡çŒ®
- **[æ›´æ–°æ—¥å¿—](./CHANGELOG.md)** - ç‰ˆæœ¬å†å²å’Œå˜æ›´è®°å½•
- **[æ¿€æ´»å‡½æ•°æ¥å£æ–‡æ¡£](./docs/activation_interface.md)** - æ¿€æ´»å‡½æ•°ä½¿ç”¨è¯´æ˜

## ğŸ¤ è´¡çŒ®

æ¬¢è¿è´¡çŒ®ä»£ç ã€æŠ¥å‘Šé—®é¢˜æˆ–æå‡ºå»ºè®®ï¼

1. Fork é¡¹ç›®
2. åˆ›å»ºç‰¹æ€§åˆ†æ”¯ (`git checkout -b feature/AmazingFeature`)
3. æäº¤æ›´æ”¹ (`git commit -m 'Add some AmazingFeature'`)
4. æ¨é€åˆ°åˆ†æ”¯ (`git push origin feature/AmazingFeature`)
5. å¼€å¯ Pull Request

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨ MIT è®¸å¯è¯ - è¯¦è§ [LICENSE](LICENSE) æ–‡ä»¶

## ğŸ™ è‡´è°¢

æœ¬é¡¹ç›®åœ¨å¼€å‘è¿‡ç¨‹ä¸­å‚è€ƒäº†ä»¥ä¸‹ä¼˜ç§€é¡¹ç›®å’Œèµ„æºï¼š

### å¼€æºé¡¹ç›®

- **[vLLM](https://github.com/vllm-project/vllm)** - é«˜æ€§èƒ½ LLM æ¨ç†å¼•æ“ï¼ŒPagedAttention å’Œ Continuous Batching çš„åˆ›æ–°å®ç°ç»™äº†æˆ‘ä»¬å¾ˆå¤§å¯å‘
- **[Flash Attention](https://github.com/Dao-AILab/flash-attention)** - é«˜æ•ˆæ³¨æ„åŠ›å®ç°çš„æ ‡æ†ï¼Œä¸ºæˆ‘ä»¬çš„ä¼˜åŒ–æ–¹å‘æä¾›äº†é‡è¦å‚è€ƒ
- **[Transformers](https://github.com/huggingface/transformers)** - Hugging Face çš„ Transformers åº“ï¼Œæä¾›äº†å®Œå–„çš„æ¨¡å‹å®ç°å’Œæƒé‡åŠ è½½æ–¹æ¡ˆ
- **[TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM)** - NVIDIA çš„æ¨ç†ä¼˜åŒ–æ–¹æ¡ˆï¼ŒCUDA kernel ä¼˜åŒ–æ€è·¯å€¼å¾—å­¦ä¹ 

### ç†è®ºåŸºç¡€

- **Attention Is All You Need** - Transformer æ¶æ„çš„å¥ åŸºè®ºæ–‡
- **GQA: Training Generalized Multi-Query Transformer Models** - Grouped Query Attention çš„è®¾è®¡æ€æƒ³
- **Efficient Memory Management for Large Language Model Serving with PagedAttention** - vLLM çš„æ ¸å¿ƒæŠ€æœ¯è®ºæ–‡
- **FlashAttention: Fast and Memory-Efficient Exact Attention** - Flash Attention ç®—æ³•

### ç‰¹åˆ«è‡´è°¢

æ„Ÿè°¢æ‰€æœ‰ä¸ºå¼€æº LLM ç”Ÿæ€ç³»ç»Ÿåšå‡ºè´¡çŒ®çš„å¼€å‘è€…å’Œç ”ç©¶è€…ã€‚æœ¬é¡¹ç›®ä½œä¸ºæ•™è‚²æ€§è´¨çš„å®ç°ï¼Œæ—¨åœ¨å¸®åŠ©æ›´å¤šäººç†è§£ç°ä»£ LLM æ¨ç†ç³»ç»Ÿçš„å·¥ä½œåŸç†ã€‚

å¦‚æœæœ¬é¡¹ç›®çš„ä»£ç ä¸­æœ‰ä»»ä½•å‚è€ƒæœªæ˜ç¡®æ ‡æ³¨æ¥æºï¼Œè¯·è”ç³»æˆ‘ä»¬ï¼Œæˆ‘ä»¬ä¼šç«‹å³è¡¥å……è¯´æ˜ã€‚

## ğŸ“š ç›¸å…³èµ„æº

- [Transformer è®ºæ–‡](https://arxiv.org/abs/1706.03762)
- [GQA è®ºæ–‡](https://arxiv.org/abs/2305.13245)
- [Flash Attention](https://arxiv.org/abs/2205.14135)
- [vLLM è®ºæ–‡](https://arxiv.org/abs/2309.06180)
- [Qwen2 æŠ€æœ¯æŠ¥å‘Š](https://arxiv.org/abs/2407.10671)

## ğŸ“§ è”ç³»æ–¹å¼

- ä½œè€…: lzy
- Email: tomlzy213@gmail.com
- GitHub: [@tom-jerr](https://github.com/tom-jerr)

---

<div align="center">

**å¦‚æœè¿™ä¸ªé¡¹ç›®å¯¹ä½ æœ‰å¸®åŠ©ï¼Œè¯·ç»™ä¸ª â­ï¸ Starï¼**

Made with â¤ï¸ by [tom-jerr](https://github.com/tom-jerr)

</div>
